{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# End-to-End XGBoost Demo (GPU, No External Imports)\nThis notebook mirrors the project pipeline\u2014data split, clustering, feature engineering, training, and inference\u2014in a **single file**. All code is inlined from the repository scripts so you can understand each step without jumping across modules or using a config file.\n\n**Defaults** (matching the repository's GPU/XGBoost setup):\n- Data root: `/workspace/oibc/data`\n- Save path: `/workspace/oibc/data/result/exp34_all_1`\n- Clusters: 10\n- Model: XGBoost (GPU accelerated if available)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import logging, warnings, os, time, pickle\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Tuple, Optional\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nimport pvlib, pytz\n\ntry:\n    from xgboost import XGBRegressor\nexcept ImportError:\n    raise ImportError(\"Please install xgboost before running this notebook.\")\n\nwarnings.filterwarnings(\"ignore\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ---------------------------------------------------------------------\n# Hard-coded configuration (mirrors src/config.yaml defaults)\n# ---------------------------------------------------------------------\nDATA_ROOT = Path(\"/workspace/oibc/data\")\nSAVE_PATH = Path(\"/workspace/oibc/data/result/exp34_all_1\")\nSPLIT_DIR = Path(\"/workspace/oibc/data_split\")\nCLUSTER_DIR = Path(\"/workspace/oibc/cluster_all\")\nSUBMISSION_FILE = DATA_ROOT / \"submission_sample.csv\"\n\nUSE_GPU = True\nN_CLUSTERS = 10\n\n# Model params (from src/main_xgb.py)\nBASE_MODEL_PARAMS = {\n    \"objective\": \"reg:squarederror\",\n    \"n_estimators\": 6000,\n    \"learning_rate\": 0.035,\n    \"max_depth\": 11,\n    \"subsample\": 0.856,\n    \"colsample_bytree\": 0.835,\n    \"random_state\": 42,\n    \"n_jobs\": -1,\n    \"verbosity\": 1,\n    \"eval_metric\": \"mae\",\n}\nGPU_MODEL_PARAMS = {\"device\": \"cuda\", \"tree_method\": \"hist\"}\n\nTARGET = \"nins\"\nCAT_FEATURES = [\"pv_id\", \"cluster\"]\nEXCLUDE_FEATURES = {\"time\", \"type\", \"energy\", TARGET}\nTIMEZONE = \"Asia/Seoul\"\nDEFAULT_LATITUDE = 36.0\nDEFAULT_LONGITUDE = 128.0\n\nSAVE_PATH.mkdir(parents=True, exist_ok=True)\n(SAVE_PATH / \"model\").mkdir(parents=True, exist_ok=True)\nCLUSTER_DIR.mkdir(parents=True, exist_ok=True)\nSPLIT_DIR.mkdir(parents=True, exist_ok=True)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# ---------------------------------------------------------------------\n# Utilities\n# ---------------------------------------------------------------------\ndef setup_logging(log_path: Path):\n    log_path.parent.mkdir(parents=True, exist_ok=True)\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        handlers=[logging.FileHandler(log_path, \"w\"), logging.StreamHandler()],\n    )\n\n\ndef check_gpu_available():\n    try:\n        import torch\n        return torch.cuda.is_available()\n    except ImportError:\n        import subprocess\n        try:\n            subprocess.check_output([\"nvidia-smi\"])\n            return True\n        except Exception:\n            return False\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Data split (train/validation)\nSplits by unique `pv_id` to avoid leakage. Mirrors `scripts/split.py` but uses the hard-coded paths above.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def split_data(seed: int = 42, ratio: float = 0.1):\n    csv_path = DATA_ROOT / \"train.csv\"\n    train_path = SPLIT_DIR / \"train_split.csv\"\n    val_path = SPLIT_DIR / \"val_split.csv\"\n\n    df = pd.read_csv(csv_path)\n    pv_ids = df[\"pv_id\"].unique()\n    train_ids, val_ids = train_test_split(\n        pv_ids, test_size=ratio, random_state=seed, shuffle=True\n    )\n    train_df = df[df[\"pv_id\"].isin(train_ids)]\n    val_df = df[df[\"pv_id\"].isin(val_ids)]\n\n    train_df.to_csv(train_path, index=False)\n    val_df.to_csv(val_path, index=False)\n\n    logging.info(f\"Training set: {len(train_df)} rows, {len(train_ids)} unique pv_ids\")\n    logging.info(f\"Validation set: {len(val_df)} rows, {len(val_ids)} unique pv_ids\")\n    return train_path, val_path\n\ntrain_split_path, val_split_path = split_data()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Clustering (train + reuse for val/test)\nFollows `scripts/add_cluster_efficient.py` and `scripts/add_cluster_from_train.py` to build cluster-enhanced features.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "SELECT_COLUMNS = [\"uv_idx\", \"nins\", \"humidity\"]\n\n\ndef build_train_clusters(train_path: Path, num_cluster: int = N_CLUSTERS):\n    df = pd.read_csv(train_path, parse_dates=[\"time\"])\n    df[\"_orig_idx\"] = np.arange(len(df))\n    orig_idx_snapshot = df[\"_orig_idx\"].copy()\n\n    if not {\"coord1\", \"coord2\"}.issubset(df.columns):\n        raise ValueError(\"Columns 'coord1' and 'coord2' are required for clustering.\")\n\n    pv_location = (\n        df[[\"pv_id\", \"coord1\", \"coord2\"]]\n        .dropna(subset=[\"coord1\", \"coord2\"])\n        .drop_duplicates(subset=[\"pv_id\"], keep=\"first\")\n    )\n\n    kmeans = KMeans(n_clusters=num_cluster, random_state=42)\n    pv_location[\"cluster\"] = kmeans.fit_predict(pv_location[[\"coord1\", \"coord2\"]]).astype(int)\n\n    joblib.dump(kmeans, CLUSTER_DIR / \"train_cluster_model.joblib\", compress=3)\n    centroids = pd.DataFrame(kmeans.cluster_centers_, columns=[\"coord1\", \"coord2\"])\n    joblib.dump(centroids, CLUSTER_DIR / \"train_cluster_centroids.joblib\", compress=3)\n\n    df = df.merge(pv_location[[\"pv_id\", \"cluster\"]], on=\"pv_id\", how=\"left\")\n    df[\"hour\"] = df[\"time\"].dt.floor(\"H\")\n\n    cluster_means = df.groupby([\"hour\", \"cluster\"])[SELECT_COLUMNS].mean().reset_index()\n    df_cluster_all = None\n    for c in range(num_cluster):\n        sub = cluster_means[cluster_means[\"cluster\"] == c].copy()\n        sub = sub.drop(columns=\"cluster\").add_prefix(f\"cluster_{c}_\")\n        sub = sub.rename(columns={f\"cluster_{c}_hour\": \"hour\"})\n        df_cluster_all = sub if df_cluster_all is None else pd.merge(\n            df_cluster_all, sub, on=\"hour\", how=\"outer\"\n        )\n\n    df = df.merge(df_cluster_all, on=\"hour\", how=\"left\")\n\n    coords = df[[\"coord1\", \"coord2\"]].to_numpy()\n    for i, (cx, cy) in enumerate(kmeans.cluster_centers_):\n        dist = np.sqrt((coords[:, 0] - cx) ** 2 + (coords[:, 1] - cy) ** 2)\n        df[f\"cluster_{i}_ratio\"] = 1 / (1 + dist) ** 2\n\n    df = (\n        df.sort_values([\"cluster\", \"pv_id\", \"hour\"])\n        .groupby(\"cluster\", group_keys=False)\n        .apply(lambda g: g.ffill().bfill())\n    )\n    df = df.sort_values(\"_orig_idx\").reset_index(drop=True).drop(columns=\"_orig_idx\")\n\n    output_path = CLUSTER_DIR / \"train_with_cluster_means_and_ratios.joblib\"\n    joblib.dump(df, output_path, compress=3)\n    return df, cluster_means, kmeans\n\n\ndef apply_clusters(df_path: Path, kmeans: KMeans, cluster_means: pd.DataFrame, prefix: str):\n    df = pd.read_csv(df_path, parse_dates=[\"time\"])\n    df[\"_orig_idx\"] = np.arange(len(df))\n    orig_idx_snapshot = df[\"_orig_idx\"].copy()\n\n    pv_location = (\n        df[[\"pv_id\", \"coord1\", \"coord2\"]]\n        .dropna(subset=[\"coord1\", \"coord2\"])\n        .drop_duplicates(subset=[\"pv_id\"], keep=\"first\")\n    )\n    df = df.merge(\n        pv_location.assign(cluster=kmeans.predict(pv_location[[\"coord1\", \"coord2\"]]).astype(int))[\n            [\"pv_id\", \"cluster\"]\n        ],\n        on=\"pv_id\",\n        how=\"left\",\n    )\n\n    num_cluster = len(kmeans.cluster_centers_)\n    df[\"hour\"] = df[\"time\"].dt.floor(\"H\")\n    df_cluster_all = None\n    for c in range(num_cluster):\n        sub = cluster_means[cluster_means[\"cluster\"] == c].copy()\n        sub = sub.drop(columns=\"cluster\").add_prefix(f\"cluster_{c}_\")\n        sub = sub.rename(columns={f\"cluster_{c}_hour\": \"hour\"})\n        df_cluster_all = sub if df_cluster_all is None else pd.merge(\n            df_cluster_all, sub, on=\"hour\", how=\"outer\"\n        )\n\n    df = df.merge(df_cluster_all, on=\"hour\", how=\"left\")\n\n    coords = df[[\"coord1\", \"coord2\"]].to_numpy()\n    for i, (cx, cy) in enumerate(kmeans.cluster_centers_):\n        dist = np.sqrt((coords[:, 0] - cx) ** 2 + (coords[:, 1] - cy) ** 2)\n        df[f\"cluster_{i}_ratio\"] = 1 / (1 + dist) ** 2\n\n    df = (\n        df.sort_values([\"cluster\", \"pv_id\", \"hour\"])\n        .groupby(\"cluster\", group_keys=False)\n        .apply(lambda g: g.ffill().bfill())\n    )\n    df = df.sort_values(\"_orig_idx\").reset_index(drop=True).drop(columns=\"_orig_idx\")\n\n    output_path = CLUSTER_DIR / f\"{prefix}_with_cluster_means_and_ratios.joblib\"\n    joblib.dump(df, output_path, compress=3)\n    return df\n\ntrain_cluster_df, cluster_means, kmeans = build_train_clusters(train_split_path)\nval_cluster_df = apply_clusters(val_split_path, kmeans, cluster_means, prefix=\"val\")\ntest_cluster_df = apply_clusters(DATA_ROOT / \"test.csv\", kmeans, cluster_means, prefix=\"test\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Feature engineering\nInlined from `src/preprocess.py` (temporal, solar, weather interactions, interpolation, and matrix building).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def engineer_features(\n    df: pd.DataFrame,\n    latitude: float = DEFAULT_LATITUDE,\n    longitude: float = DEFAULT_LONGITUDE,\n) -> pd.DataFrame:\n    logging.info(\"Engineering features...\")\n    if df[\"time\"].dt.tz is None:\n        df[\"time\"] = df[\"time\"].dt.tz_localize(TIMEZONE)\n\n    original_index = df.index\n    sort_keys = [\"pv_id\", \"time\"] if \"pv_id\" in df.columns else [\"time\"]\n    df = df.sort_values(sort_keys)\n\n    df = add_temporal_features(df)\n    df = add_solar_features(df, latitude, longitude)\n    df = add_weather_interactions(df)\n    df = interpolate_weather(df)\n\n    df = df.loc[original_index].reset_index(drop=True)\n    logging.info(f\"  Total features after engineering: {len(df.columns)}\")\n    return df\n\n\ndef add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n    minute_of_day = df[\"time\"].dt.hour * 60 + df[\"time\"].dt.minute\n    df[\"hour_sin\"] = np.sin(2 * np.pi * minute_of_day / 1440)\n    df[\"hour_cos\"] = np.cos(2 * np.pi * minute_of_day / 1440)\n    df[\"day_of_year\"] = df[\"time\"].dt.dayofyear\n    df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365.25)\n    df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365.25)\n    df[\"week_of_year\"] = df[\"time\"].dt.isocalendar().week.astype(int)\n    df[\"is_weekend\"] = (df[\"time\"].dt.weekday >= 5).astype(np.int8)\n    df[\"month\"] = df[\"time\"].dt.month.astype(np.int8)\n    df[\"season\"] = ((df[\"month\"] % 12 + 3) // 3).astype(np.int8)\n    df[\"minute_of_day\"] = minute_of_day.astype(np.int16)\n    return df\n\n\ndef add_solar_features(df: pd.DataFrame, latitude: float, longitude: float) -> pd.DataFrame:\n    times = df[\"time\"]\n    if times.dt.tz is None:\n        times = times.dt.tz_localize(TIMEZONE)\n    solar = pvlib.solarposition.get_solarposition(\n        times.dt.tz_convert(pytz.utc), latitude=latitude, longitude=longitude\n    )\n    df[\"solar_altitude\"] = solar[\"apparent_elevation\"].clip(lower=0).values\n    df[\"solar_azimuth\"] = solar[\"azimuth\"].values\n    df[\"sun_up\"] = (df[\"solar_altitude\"] > 0).astype(np.int8)\n    df[\"solar_altitude_norm\"] = df[\"solar_altitude\"] / 90.0\n    return df\n\n\ndef add_weather_interactions(df: pd.DataFrame) -> pd.DataFrame:\n    if {\"temp_max\", \"temp_min\"}.issubset(df.columns):\n        df[\"temp_range\"] = df[\"temp_max\"] - df[\"temp_min\"]\n    if {\"temp_a\", \"temp_b\"}.issubset(df.columns):\n        df[\"temp_diff_surface\"] = df[\"temp_a\"] - df[\"temp_b\"]\n    if {\"pressure\", \"ground_press\"}.issubset(df.columns):\n        df[\"pressure_gap\"] = df[\"pressure\"] - df[\"ground_press\"]\n    if {\"real_feel_temp\", \"real_feel_temp_shade\"}.issubset(df.columns):\n        df[\"feels_gap\"] = df[\"real_feel_temp\"] - df[\"real_feel_temp_shade\"]\n    if {\"wind_spd_a\", \"wind_spd_b\"}.issubset(df.columns):\n        df[\"wind_resultant\"] = np.hypot(\n            df[\"wind_spd_a\"].fillna(0), df[\"wind_spd_b\"].fillna(0)\n        )\n    if \"wind_gust_spd\" in df.columns:\n        df[\"gust_ratio\"] = df[\"wind_gust_spd\"] / (df[\"wind_resultant\"] + 1e-3)\n    if {\"cloud_a\", \"cloud_b\"}.issubset(df.columns):\n        df[\"cloud_cover_mean\"] = df[[\"cloud_a\", \"cloud_b\"]].mean(axis=1)\n    if {\"temp_a\", \"dew_point\"}.issubset(df.columns):\n        df[\"dewpoint_spread\"] = df[\"temp_a\"] - df[\"dew_point\"]\n    if {\"humidity\", \"rel_hum\"}.issubset(df.columns):\n        rel = df[\"rel_hum\"].replace(0, np.nan)\n        df[\"humidity_ratio\"] = df[\"humidity\"] / rel\n    if \"precip_1h\" in df.columns:\n        df[\"is_rainy\"] = (df[\"precip_1h\"].fillna(0) > 0).astype(np.int8)\n    return df\n\n\ndef interpolate_weather(df: pd.DataFrame) -> pd.DataFrame:\n    weather_cols = [\n        c\n        for c in df.select_dtypes(include=[np.number]).columns\n        if c not in EXCLUDE_FEATURES and c not in CAT_FEATURES\n    ]\n    if \"pv_id\" in df.columns and weather_cols:\n        df[weather_cols] = df.groupby(\"pv_id\")[weather_cols].bfill().ffill()\n    return df\n\n\ndef align_feature_frames(train_df: pd.DataFrame, test_df: pd.DataFrame, cat_features: List[str]):\n    train_filled = train_df.copy()\n    test_filled = test_df.copy()\n\n    common_numeric = [\n        c\n        for c in train_filled.select_dtypes(include=[np.number]).columns\n        if c in test_filled.columns\n    ]\n    if common_numeric:\n        medians = train_filled[common_numeric].median().fillna(0)\n        train_filled[common_numeric] = train_filled[common_numeric].fillna(medians)\n        test_filled[common_numeric] = test_filled[common_numeric].fillna(medians)\n\n    for col in cat_features:\n        if col not in train_filled.columns or col not in test_filled.columns:\n            continue\n        train_filled[col] = train_filled[col].astype(\"category\")\n        test_filled[col] = test_filled[col].astype(\"category\")\n        mode_value = train_filled[col].mode(dropna=True)\n        fill_value = mode_value.iloc[0] if not mode_value.empty else \"missing\"\n        if fill_value not in train_filled[col].cat.categories:\n            train_filled[col] = train_filled[col].cat.add_categories([fill_value])\n        if fill_value not in test_filled[col].cat.categories:\n            test_filled[col] = test_filled[col].cat.add_categories([fill_value])\n        train_filled[col] = train_filled[col].fillna(fill_value)\n        test_filled[col] = test_filled[col].fillna(fill_value)\n\n    return train_filled, test_filled\n\n\ndef build_feature_matrix(train_df: pd.DataFrame, test_df: pd.DataFrame):\n    logging.info(\"Building feature matrices...\")\n    feature_cols = [c for c in train_df.columns if c not in EXCLUDE_FEATURES]\n    cat_features = [c for c in CAT_FEATURES if c in feature_cols]\n    train_features = train_df[feature_cols]\n    test_features = test_df[[c for c in feature_cols if c in test_df.columns]]\n    train_features, test_features = align_feature_frames(\n        train_features, test_features, cat_features\n    )\n    return train_features, test_features, feature_cols, cat_features\n\n\ndef encode_for_xgb(df: pd.DataFrame, cat_features: List[str]) -> pd.DataFrame:\n    if not cat_features:\n        return df\n    for col in cat_features:\n        df[col] = df[col].cat.codes.astype(\"int16\")\n    return df\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Process datasets & build matrices\nRuns feature engineering for train/val/test, drops the auxiliary `hour` column, aligns categories, and encodes for XGBoost.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def process_split(df: pd.DataFrame, name: str):\n    logging.info(f\"Processing {name}\u2026\")\n    df[\"time\"] = pd.to_datetime(df[\"time\"])\n    df = engineer_features(df)\n    if name == \"train\":\n        df = df.dropna(subset=[TARGET])\n    if \"hour\" in df.columns:\n        df.drop(columns=[\"hour\"], inplace=True)\n    return df\n\ntrain_processed = process_split(train_cluster_df.copy(), \"train\")\nval_processed = process_split(val_cluster_df.copy(), \"val\")\ntest_processed = process_split(test_cluster_df.copy(), \"test\")\n\nfor df in (train_processed, val_processed, test_processed):\n    if \"pv_id\" in df.columns:\n        df[\"pv_id\"] = df[\"pv_id\"].astype(\"category\")\n    if \"cluster\" in df.columns:\n        df[\"cluster\"] = df[\"cluster\"].astype(\"category\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Train GPU XGBoost and evaluate\nMirrors `src/main_xgb.py` (single model). Uses GPU if available.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_model(use_gpu: bool):\n    params = BASE_MODEL_PARAMS.copy()\n    if use_gpu:\n        params.update(GPU_MODEL_PARAMS)\n    return XGBRegressor(**params)\n\n\ngpu = check_gpu_available() and USE_GPU\nif USE_GPU and not gpu:\n    logging.warning(\"GPU requested but not available \u2014 running on CPU.\")\n\nlogging.info(\"Building feature matrices\u2026\")\ntrain_features, val_features, feature_cols, cat_features = build_feature_matrix(\n    train_processed, val_processed\n)\ntrain_features, test_features, _, _ = build_feature_matrix(train_processed, test_processed)\n\nif \"pv_id\" in feature_cols:\n    feature_cols = [c for c in feature_cols if c != \"pv_id\"]\nif \"pv_id\" in cat_features:\n    cat_features = [c for c in cat_features if c != \"pv_id\"]\n\nX_train = train_features[feature_cols]\nX_val = val_features[feature_cols]\nX_test = test_features[feature_cols]\n\ny_train = train_processed[TARGET].clip(lower=0).values\ny_val = val_processed[TARGET].clip(lower=0).values\ny_val_raw = val_processed[TARGET].values\n\nxgb_data = {\n    k: encode_for_xgb(v.copy(), cat_features)\n    for k, v in zip([\"xgb_train\", \"xgb_val\", \"xgb_test\"], [X_train, X_val, X_test])\n}\n\nlogging.info(\"Training XGBoost\u2026\")\nstart_time = time.time()\nmodel = build_model(use_gpu=gpu)\nmodel.fit(xgb_data[\"xgb_train\"], y_train, eval_set=[(xgb_data[\"xgb_val\"], y_val)], verbose=100)\nval_preds = np.maximum(model.predict(xgb_data[\"xgb_val\"]), 0)\ntest_preds = np.maximum(model.predict(xgb_data[\"xgb_test\"]), 0)\n\nmodel_path = SAVE_PATH / \"model\" / \"xgboost.pkl\"\nwith open(model_path, \"wb\") as f:\n    pickle.dump(model, f)\nlogging.info(f\"XGBoost training finished in {time.time() - start_time:.2f}s\")\n\nmae = mean_absolute_error(y_val_raw, val_preds)\nlogging.info(f\"Validation MAE: {mae:.6f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Inference & submission file\nGenerates predictions for the public test set using the trained model and writes a submission CSV next to the sample file.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "submission_path = DATA_ROOT / \"submission.csv\"\nif SUBMISSION_FILE.exists():\n    submission = pd.read_csv(SUBMISSION_FILE)\n    if \"prediction\" in submission.columns:\n        submission[\"prediction\"] = test_preds\n    elif \"nins\" in submission.columns:\n        submission[\"nins\"] = test_preds\n    else:\n        submission = submission.assign(prediction=test_preds)\nelse:\n    submission = pd.DataFrame({\"prediction\": test_preds})\n\nsubmission.to_csv(submission_path, index=False)\nlogging.info(f\"Saved submission to {submission_path}\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}