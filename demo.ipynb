{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OIBC Submission Pipeline - Demonstration Notebook\n",
    "\n",
    "This notebook demonstrates the complete workflow for the OIBC (Open Innovation Big Competition) submission.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "The pipeline consists of 5 main steps:\n",
    "1. **Data Splitting**: Split data into train/validation sets\n",
    "2. **Clustering**: Create location-based PV clusters\n",
    "3. **Feature Engineering**: Generate cluster-based features\n",
    "4. **Model Training**: Train ensemble or individual models\n",
    "5. **Inference**: Generate predictions\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "You can run the entire pipeline using the orchestration script:\n",
    "\n",
    "```bash\n",
    "python run_pipeline.py --config p34/config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('p34/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Exploration\n",
    "\n",
    "Let's explore the training data to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (update path as needed)\n",
    "train_path = config.get('train_split_path', '/workspace/oibc/data/train.csv')\n",
    "\n",
    "# Read a sample of the data\n",
    "df = pd.read_csv(train_path, nrows=10000, parse_dates=['time'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize PV Locations\n",
    "\n",
    "Visualize the geographic distribution of PV (photovoltaic) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique PV locations\n",
    "pv_locations = df[['pv_id', 'coord1', 'coord2']].drop_duplicates()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pv_locations['coord1'], pv_locations['coord2'], alpha=0.5)\n",
    "plt.xlabel('Coordinate 1 (Longitude)')\n",
    "plt.ylabel('Coordinate 2 (Latitude)')\n",
    "plt.title(f'PV System Locations (n={len(pv_locations)})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clustering Visualization\n",
    "\n",
    "If clustering has been performed, visualize the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cluster model if available\n",
    "cluster_model_path = 'train_cluster_model.joblib'\n",
    "\n",
    "if Path(cluster_model_path).exists():\n",
    "    kmeans = joblib.load(cluster_model_path)\n",
    "    \n",
    "    # Predict clusters for PV locations\n",
    "    pv_locations['cluster'] = kmeans.predict(pv_locations[['coord1', 'coord2']])\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        pv_locations['coord1'], \n",
    "        pv_locations['coord2'],\n",
    "        c=pv_locations['cluster'],\n",
    "        cmap='tab10',\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "    \n",
    "    # Plot cluster centers\n",
    "    centers = kmeans.cluster_centers_\n",
    "    plt.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        c='red',\n",
    "        marker='X',\n",
    "        s=200,\n",
    "        edgecolors='black',\n",
    "        label='Cluster Centers'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Coordinate 1 (Longitude)')\n",
    "    plt.ylabel('Coordinate 2 (Latitude)')\n",
    "    plt.title(f'PV Clustering ({kmeans.n_clusters} clusters)')\n",
    "    plt.colorbar(scatter, label='Cluster ID')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pv_locations['cluster'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of PV Systems')\n",
    "    plt.title('PV Systems per Cluster')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cluster model not found. Run clustering step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Individual Pipeline Steps\n",
    "\n",
    "You can run individual steps of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run data splitting\n",
    "# !python data_split/split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustering\n",
    "# !python cluster_code/add_cluster_efficient.py\n",
    "# !python cluster_code/add_cluster_from_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model training\n",
    "# !cd p34 && python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Model Results\n",
    "\n",
    "After training, analyze the model performance and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions if available\n",
    "save_path = Path(config.get('save_path', './output'))\n",
    "prediction_files = list(save_path.glob('predictions_*.csv'))\n",
    "\n",
    "if prediction_files:\n",
    "    print(\"Available prediction files:\")\n",
    "    for i, pred_file in enumerate(prediction_files, 1):\n",
    "        print(f\"  {i}. {pred_file.name}\")\n",
    "    \n",
    "    # Load the first prediction file\n",
    "    predictions = pd.read_csv(prediction_files[0])\n",
    "    print(f\"\\nLoaded predictions from: {prediction_files[0].name}\")\n",
    "    print(f\"Shape: {predictions.shape}\")\n",
    "    predictions.head()\n",
    "else:\n",
    "    print(\"No prediction files found. Run the inference step first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distribution\n",
    "if prediction_files:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(predictions.iloc[:, -1], bins=50, edgecolor='black')\n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    predictions.iloc[:, -1].plot(kind='box')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    plt.title('Box Plot of Predictions')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPrediction statistics:\")\n",
    "    print(predictions.iloc[:, -1].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Complete Pipeline\n",
    "\n",
    "Run the entire pipeline using the orchestration script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline (uncomment to execute)\n",
    "# !python run_pipeline.py --config p34/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only training and inference (skip data prep)\n",
    "# !python run_pipeline.py --config p34/config.yaml --skip-split --skip-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a specific step\n",
    "# !python run_pipeline.py --config p34/config.yaml --step train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Data exploration and visualization\n",
    "2. PV location clustering visualization\n",
    "3. Running individual pipeline steps\n",
    "4. Analyzing model predictions\n",
    "5. Using the orchestration script for complete pipeline execution\n",
    "\n",
    "For production runs, use the `run_pipeline.py` script for automated execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
